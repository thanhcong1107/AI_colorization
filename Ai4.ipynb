{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macLeHoang/BTL-AI-AI-Colorization/blob/main/Ai4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjREKzKUW_8R"
      },
      "outputs": [],
      "source": [
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "#   print('and then re-execute this cell.')\n",
        "# else:\n",
        "#   print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LE3af0grS7Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rjiOf748LQO"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/qubvel/classification_models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfZkk0yBRhYm"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/test2017.zip\n",
        "!unzip -qq -o test2017.zip\n",
        "!rm test2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq4__23orPnH"
      },
      "outputs": [],
      "source": [
        "from skimage.color import rgb2lab, lab2rgb\n",
        "from skimage import transform\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from classification_models.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjU0w8lYLUP"
      },
      "source": [
        "# DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoIdxc0hW6NX"
      },
      "outputs": [],
      "source": [
        "path = '/content/test2017'\n",
        "c = 0\n",
        "for f in tqdm(os.listdir(path)):\n",
        "  fPath = os.path.join(path, f)\n",
        "  img = Image.open(fPath)\n",
        "  if img.mode != 'RGB':\n",
        "    os.remove(fPath)\n",
        "    c += 1\n",
        "\n",
        "print()\n",
        "print(f'Remove {c} gray images')\n",
        "print(f'Remain {len(os.listdir(path))} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8MLc6KqcW-i"
      },
      "outputs": [],
      "source": [
        "SIZE = (256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRCQmcShRGLH"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.list_files('/content/test2017/*.jpg')\n",
        "\n",
        "def process(path):\n",
        "  path_ = bytes.decode(path.numpy())\n",
        "\n",
        "  img = Image.open(path_)\n",
        "  img = img.resize(SIZE, Image.BICUBIC)\n",
        "  \n",
        "  # Slightly augmentation\n",
        "  randNumber = random.random() # create random number in range [0, 1]\n",
        "  if randNumber > 0.7: # do augmentation if the number created is greater than 0.5\n",
        "    anotherRandNumber = random.random() \n",
        "    if anotherRandNumber < 0.5:\n",
        "      img = img.transpose(Image.FLIP_LEFT_RIGHT) # flip vertical\n",
        "      img = np.array(img)\n",
        "\n",
        "    elif 0.5 < anotherRandNumber:\n",
        "      img = img.transpose(Image.FLIP_TOP_BOTTOM) # flip horizontal\n",
        "      img = np.array(img)\n",
        "\n",
        "    # elif 0.7 < anotherRandNumber:\n",
        "    #   alpha = random.randint(-30, 30) \n",
        "    #   img = img.rotate(alpha, expand = False) # rotate\n",
        "    #   img = np.array(img)\n",
        "\n",
        "    # elif anotherRandNumber > 0.9:\n",
        "    #   img = np.array(img)\n",
        "    #   sx = random.uniform(-0.2, 0.2) #create random number in range [-0.2, 0.2]\n",
        "    #   sy = random.uniform(-0.2, 0.2)\n",
        "    #   matrix = np.asarray([[1, sx, 0], [sy, 1, 0], [0, 0, 1]])\n",
        "    #   affine = transform.AffineTransform(matrix)\n",
        "    #   img = transform.warp(img, affine.params) # shere\n",
        "  else:\n",
        "    img = np.array(img)\n",
        "\n",
        "  labImg = rgb2lab(img)\n",
        "  lChannel = labImg[:, :, 0:1] / 50.0 - 1 # convert L channel to range [-1, 1]\n",
        "  abChannels = labImg[:, :, 1:] / 110.0 # convert ab channel to range [-1, 1]\n",
        "\n",
        "  return tf.convert_to_tensor(lChannel, dtype = tf.float32), \\\n",
        "         tf.convert_to_tensor(abChannels, dtype = tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4k4liQtipLv"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: tf.py_function(process, [x], [tf.float32, tf.float32]))\n",
        "dataset = dataset.batch(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEajJo6Z0Mqf"
      },
      "outputs": [],
      "source": [
        "# l, ab = next(iter(dataset))\n",
        "\n",
        "# plt.imshow(l[0, :, :, 0].numpy(),cmap = 'gray')\n",
        "# np.max(ab[0, :, :, 0].numpy()), np.min(ab[0, :, :, 0].numpy()), np.max(ab[0, :, :, 1].numpy()), np.min(ab[0, :, :, 1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIbdWH6TX_mh"
      },
      "source": [
        "# GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1BJxbUqBMjz"
      },
      "outputs": [],
      "source": [
        "def process(input_, nfilters_1 = 1024, nfilters_2 = 512, ksize = (3, 3), strides = 1, last_relu = True):\n",
        "  x = tf.keras.layers.ZeroPadding2D()(input_)\n",
        "  x = tf.keras.layers.Conv2D(nfilters_1, ksize, strides)(x)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "  x = tf.keras.layers.ZeroPadding2D()(x)\n",
        "  x = tf.keras.layers.Conv2D(nfilters_2, ksize, strides)(x)\n",
        "\n",
        "  if last_relu:\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn-azIB-Wjog"
      },
      "outputs": [],
      "source": [
        "def decoder(input_, concat, nfilters = 1024, ksize = (1, 1), strides = 1):\n",
        "  x = tf.keras.layers.Conv2D(nfilters, ksize, strides)(input_)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "  out = tf.nn.depth_to_space(x, 2)\n",
        "  x = tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(out)\n",
        "  x = tf.keras.layers.Concatenate()([concat, x])\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "  \n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ql_zO7Qr6g"
      },
      "outputs": [],
      "source": [
        "class GENERATOR(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ResNet18, _ = Classifiers.get('resnet18')\n",
        "  \n",
        "  def __call__(self):\n",
        "    resnet18 = self.ResNet18(input_shape = (256, 256, 3), \n",
        "                             weights = 'imagenet',\n",
        "                             include_top = False)\n",
        "  \n",
        "    encoder_1 = resnet18.get_layer('bn0').output # encode 128 - shape = (None, 128, 128, 64)\n",
        "    encoder_2 = resnet18.get_layer('stage2_unit1_bn1').output # encode 64 - shape = (None, 64, 64, 64)\n",
        "    encoder_3 = resnet18.get_layer('stage3_unit1_bn1').output # encode 32 - shape = (None, 32, 32, 128)\n",
        "    encoder_4 = resnet18.get_layer('stage4_unit1_bn1').output # encode 16 - shape = (None, 16, 16, 256)\n",
        "\n",
        "    last_layer = resnet18.layers[-1].output\n",
        "    bridge = process(last_layer)\n",
        "\n",
        "    x = decoder(bridge, encoder_4) # shape = (None, 16, 16, 512)\n",
        "    x = process(x, 512, 512)\n",
        "    x = decoder(x, encoder_3, 1024) # shape = (None, 32, 32, 384)\n",
        "    x = process(x, 384, 384)\n",
        "    x = decoder(x, encoder_2, 768) # shape = (None, 64, 64, 256)\n",
        "    x = process(x, 256, 256)\n",
        "    x = decoder(x, encoder_1, 512) # shape = (None, 128, 128, 192)\n",
        "    x = process(x, 96, 96)\n",
        "    x = tf.keras.layers.Conv2D(384, (1, 1), 1)(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.nn.depth_to_space(x, 2) # shape = (256, 256, 96)\n",
        "    x = tf.keras.layers.Concatenate()([x, resnet18.input[:, :, :, 0:1]]) # shape = (None, 256, 256, 97)\n",
        "    res = process(x, 97, 97, last_relu = False)\n",
        "    x = tf.keras.layers.Add()([x, res])\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(2, (1, 1), 1)(x)\n",
        "    x = tf.keras.layers.Activation('tanh')(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs = resnet18.input, outputs = x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv7pPa46yDNz"
      },
      "outputs": [],
      "source": [
        "gModel = GENERATOR()\n",
        "generator = gModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vjnSjst_VYg"
      },
      "outputs": [],
      "source": [
        "# generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF80KxNZ45w"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK5HBA9E73N0"
      },
      "outputs": [],
      "source": [
        "def generative_loss(target, predict, discriminator_output_of_predict, LAMBDA = 100.0):\n",
        "  l1_loss = tf.reduce_mean(tf.abs(predict - target))\n",
        "\n",
        "  generative_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True)(tf.ones_like(discriminator_output_of_predict),\n",
        "                                                                           discriminator_output_of_predict)\n",
        "  total_loss = generative_loss + LAMBDA*l1_loss\n",
        "  return total_loss, generative_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-0UDUaUZgJS"
      },
      "source": [
        "# DISCRIMINATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxP5tXy9vJN1"
      },
      "outputs": [],
      "source": [
        "initializer = tf.random_normal_initializer(0.0, 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHpRGoGOhABo"
      },
      "outputs": [],
      "source": [
        "def down_sample(input, nums_filters, kernel_size = (4, 4), strides = 2, use_batchnorm = True, **kwags):\n",
        "  x = input\n",
        "  x = tf.keras.layers.Conv2D(nums_filters, \n",
        "                             kernel_size = kernel_size,\n",
        "                             strides = strides,\n",
        "                             kernel_initializer = initializer,\n",
        "                             use_bias = False,\n",
        "                             padding = 'same',\n",
        "                             **kwags)(x)\n",
        "  if use_batchnorm:\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5)(x)\n",
        "  \n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1NdGwOEyvtZ"
      },
      "outputs": [],
      "source": [
        "class DISCRIMINATOR(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__\n",
        "    self.cpart= tf.keras.layers.Input(shape = (None, None, 1)) # use as condition input\n",
        "    self.predict = tf.keras.layers.Input(shape = (None, None, 2))\n",
        "  \n",
        "  def __call__(self):\n",
        "    x = tf.keras.layers.Concatenate(axis = -1)([self.cpart, self.predict]) # shape = None, 256, 256, 3\n",
        "\n",
        "    x = down_sample(x, 64, use_batchnorm = False) # shape = None, 128, 128, 64\n",
        "    x = down_sample(x, 128) # shape = None, 64, 64, 128\n",
        "    x = down_sample(x, 256) # shape = None, 32, 32, 256\n",
        "\n",
        "    x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)))(x) # shape = None, 34, 34, 256\n",
        "    x = down_sample(x, 512, strides = 1) # shape = None, 31, 31, 512\n",
        "\n",
        "    x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)))(x) # shape = None, 33, 33, 512\n",
        "\n",
        "    # Each pixel in the feature map looks up to 70*70 patch of the origin image\n",
        "    x = tf.keras.layers.Conv2D(1, kernel_size = (4, 4), \n",
        "                               strides = 1, \n",
        "                               kernel_initializer = initializer)(x) # shape = None, 30, 30, 1\n",
        "    return tf.keras.models.Model(inputs = [self.cpart, self.predict], outputs = x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEkXWeSd4MiK"
      },
      "outputs": [],
      "source": [
        "dModel = DISCRIMINATOR()\n",
        "discriminator = dModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPokmZDaAjCz"
      },
      "outputs": [],
      "source": [
        "def discriminative_loss(target, predict):\n",
        "  posLoss = tf.keras.losses.BinaryCrossentropy(from_logits = True)(tf.ones_like(target),\n",
        "                                                                   target)\n",
        "  \n",
        "  negLoss = tf.keras.losses.BinaryCrossentropy(from_logits = True)(tf.zeros_like(predict),\n",
        "                                                                   predict)\n",
        "  return posLoss + negLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UroNKBTWaSl_"
      },
      "source": [
        "# Pre-Trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOC66TrKaXgi"
      },
      "outputs": [],
      "source": [
        "# def pretrained_loss(target, predict):\n",
        "#   l1 = tf.reduce_mean(tf.abs(target - predict))\n",
        "#   return l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmMeAO0apNr"
      },
      "outputs": [],
      "source": [
        "# pretrain_opt = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAYOKYItc3R7"
      },
      "outputs": [],
      "source": [
        "# log_dir = '/content/gdrive/MyDrive/AI_color_weights/Logs/Pretrain_Logs/'\n",
        "# summary_writer = tf.summary.create_file_writer(\n",
        "#   log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9OfGMMwf2V_",
        "outputId": "cd4593c0-d161-4d6a-9814-fe18f505a554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/AI_color_weights/Generator_v2/pre_generator-20220614-153134\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1410061c50>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# gen_ckpt_list_dir = '/content/gdrive/MyDrive/AI_color_weights/Generator_v2'\n",
        "# gen_latest_checkpoint = tf.train.latest_checkpoint(gen_ckpt_list_dir)\n",
        "# print(gen_latest_checkpoint)\n",
        "# generator.load_weights(gen_latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahNCq_bYa9af"
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "# def pre_step(L_target, ab_target, step):\n",
        "#   with tf.GradientTape() as preTape:\n",
        "#     L = tf.repeat(L_target, repeats = 3, axis = 3)\n",
        "#     ab_predict = generator(L, training = True)\n",
        "#     l1 = pretrained_loss(ab_target, ab_predict)\n",
        "  \n",
        "#   grads = preTape.gradient(l1, generator.trainable_variables)\n",
        "#   pretrain_opt.apply_gradients(zip(grads,\n",
        "#                                    generator.trainable_variables))\n",
        "  \n",
        "#   with summary_writer.as_default():\n",
        "#     tf.summary.scalar('L1_pretrained_Loss', l1, step = step//10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJMcanxPdheH"
      },
      "outputs": [],
      "source": [
        "# def pre_fit(dataset, epochs):\n",
        "#   for epoch in range(epochs):\n",
        "#     for idx, (L, ab) in tqdm(dataset.enumerate()):\n",
        "#       pre_step(L, ab, idx)\n",
        "    \n",
        "#     gen_ckpt_dir = '/content/gdrive/MyDrive/AI_color_weights/Generator_v2'\n",
        "#     gen_ckpt_name = 'pre_generator-' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "#     generator.save_weights(os.path.join(gen_ckpt_dir, gen_ckpt_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4FcFqmIip1h",
        "outputId": "1863d577-d41d-4a66-be43-eca48c631a6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2540/2540 [36:51<00:00,  1.15it/s]\n"
          ]
        }
      ],
      "source": [
        "# pre_fit(dataset, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUOZ65mfLT8"
      },
      "source": [
        "# Train Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZR0xIP1tEuM"
      },
      "outputs": [],
      "source": [
        "log_dir = '/content/gdrive/MyDrive/AI_color_weights/Logs/Train_Logs/'\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZBHamzA9i4_"
      },
      "outputs": [],
      "source": [
        "gOpt = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)\n",
        "dOpt = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMfFS0UDKMia"
      },
      "outputs": [],
      "source": [
        "# gen_ckpt_list_dir = '/content/gdrive/MyDrive/AI_color_weights/Generator_v2'\n",
        "# gen_lastest_checkpoint = tf.train.latest_checkpoint(gen_ckpt_list_dir)\n",
        "# print(gen_lastest_checkpoint)\n",
        "# generator.load_weights(gen_lastest_checkpoint)\n",
        "\n",
        "# dis_ckpt_list_dir = '/content/gdrive/MyDrive/AI_color_weights/Discriminator_v2'\n",
        "# dis_lastest_checkpoint = tf.train.latest_checkpoint(dis_ckpt_list_dir)\n",
        "# print(dis_lastest_checkpoint)\n",
        "# discriminator.load_weights(dis_lastest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWFEjkrb9F9S"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(Limg, ab_target, step):\n",
        "  with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "    L = tf.repeat(Limg, 3, axis = 3)\n",
        "    ab_predict = generator(L, training = True) # ab Generative image\n",
        "\n",
        "    d_predict = discriminator([Limg, ab_predict], training = True) # Discriminator output of predict\n",
        "    d_target = discriminator([Limg, ab_target], training = True) # Discriminator output of target\n",
        "\n",
        "    # Discriminative Loss\n",
        "    d_loss = discriminative_loss(d_target, d_predict)\n",
        "\n",
        "    # Generative loss\n",
        "    g_loss, g_gan_loss = generative_loss(ab_target, ab_predict, d_predict)\n",
        "\n",
        "  gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "  dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "  gOpt.apply_gradients(zip(gGradients,\n",
        "                           generator.trainable_variables))\n",
        "  dOpt.apply_gradients(zip(dGradients,\n",
        "                           discriminator.trainable_variables))\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('Total Gen loss', g_loss, step = step//10)\n",
        "    tf.summary.scalar('Gan loss', g_gan_loss, step = step//10)\n",
        "    tf.summary.scalar('Total Disc loss', d_loss, step = step//10)\n",
        "    # tf.summary.scalar('Positive Disc loss', pos_d_loss, step = step//10)\n",
        "    # tf.summary.scalar('Negative Disc loss', neg_d_loss, step = step//10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6iB4iPkCx8e"
      },
      "outputs": [],
      "source": [
        "def fit(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for idx, (L, ab) in tqdm(dataset.enumerate()):\n",
        "      train_step(L, ab, idx)\n",
        "\n",
        "    # save generator weights and discriminator weights after each epochs\n",
        "    gen_ckpt_dir = '/content/gdrive/MyDrive/AI_color_weights/Generator_v2'\n",
        "    gen_ckpt_name = 'generator-' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    generator.save_weights(os.path.join(gen_ckpt_dir, gen_ckpt_name))\n",
        "\n",
        "    dis_ckpt_dir = '/content/gdrive/MyDrive/AI_color_weights/Discriminator_v2'\n",
        "    dis_ckpt_name = 'discriminator-' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    discriminator.save_weights(os.path.join(dis_ckpt_dir, dis_ckpt_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYsfbjiSIZtf"
      },
      "outputs": [],
      "source": [
        "# fit \n",
        "fit(dataset, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJM1QVbWxcfj"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/gdrive/MyDrive/AI_color_weights/Logs/Train_Logs/20220627-014237"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UroNKBTWaSl_"
      ],
      "name": "Ai4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIXTnMh/gozHKudPLuTZV5",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}